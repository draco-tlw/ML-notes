## **Cost Function Intuition ðŸ§ **

The primary goal of a learning algorithm is to find the parameters (`w` and `b`) that make the cost function $J(w,b)$ as small as possible. This process is formally written as finding the **minimum** of the cost function:

$$\min_{w,b} J(w,b)$$

---

### **A Simplified Model for Visualization**

To make it easier to visualize how the cost function works, we can temporarily simplify the linear model by removing the `b` parameter (or setting `b=0`).

- **Original Model:** $f_{w,b}(x) = wx + b$
- **Simplified Model:** $f_w(x) = wx$
- **Simplified Cost Function:** $J(w) = \frac{1}{2m} \sum_{i=1}^{m} (f_w(x^{(i)}) - y^{(i)})^2 = \frac{1}{2m} \sum_{i=1}^{m} (wx^{(i)} - y^{(i)})^2$

With this simplification, the model is a line that is forced to pass through the origin (0,0), and we only need to find the best value for a single parameter, `w`.

---

### **Relating the Model to the Cost Function**

It's crucial to understand the relationship between the model plot and the cost function plot.

- **Model Plot (f vs. x):** This shows the relationship between the input `x` and the output `y`. A specific value of `w` defines a specific line on this plot.
- **Cost Function Plot (J vs. w):** This shows how "good" a particular parameter `w` is. The horizontal axis is the parameter `w`, and the vertical axis is the cost $J(w)$ associated with that `w`.

For every possible line on the model plot, there is a corresponding point on the cost function plot.

---

### **How the Cost `J(w)` Changes with `w`**

Let's trace how the cost changes for different values of `w` using a simple training set: `(1,1), (2,2), (3,3)`.

1. **If w = 1:**

   - The model is $f(x) = 1x$, which is a perfect fit for the data.
   - The error $(\hat{y} - y)$ is 0 for every data point.
   - The cost **$J(1) = 0$**. This is the lowest possible cost.

2. **If w = 0.5:**

   - The model is $f(x) = 0.5x$. This line is below the data points.
   - There is a vertical distance (an error) between each data point and the line.
   - The cost **$J(0.5)$** is a positive number (calculated as â‰ˆ 0.58).

3. **If w = 0:**
   - The model is $f(x) = 0x$, a flat horizontal line.
   - The errors are even larger.
   - The cost **$J(0)$** is higher still (calculated as â‰ˆ 2.33).

By trying many different values for `w` and plotting the resulting cost `J(w)`, we can trace out the shape of the cost function. For linear regression, this shape is a **bowl-shaped curve** (a parabola).

The **goal of linear regression** is to find the value of `w` (and `b` in the full model) that is at the very bottom of this bowl, as this corresponds to the model with the lowest possible error.
